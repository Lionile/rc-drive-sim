episodes: 1000
total_steps: null  # Alternative to episodes - use either episodes OR total_steps
buffer_size: 25000
batch_size: 256
gamma: 0.99
tau: 0.002
actor_lr: 0.000075
critic_lr: 0.0002

# Separate noise parameters for exploration and target policy smoothing
explore_sigma_start: 0.3    # Starting exploration noise (decays over episodes)
explore_sigma_end: 0.05     # Final exploration noise (reached at max episodes)
explore_clip: 0.3           # Clip value for exploration noise (constant)
smooth_sigma: 0.2           # Noise for target policy smoothing (TD3 specific)
smooth_clip: 0.5            # Clip value for target policy smoothing noise

policy_delay: 2
warmup_steps: 2000
save_every_episodes: 10  # Save a checkpoint every N episodes (0 to disable)

# Epsilon-greedy exploration decay
epsilon_start: 0  # Starting epsilon value for exploration
epsilon_end: 0   # Final epsilon value (reached at max episodes)

# Environment settings
map: [6, 7]
physics_fps: 50  # Physics simulation frequency (Hz) - affects dt calculations
allow_reverse: false  # Allow neural network to output negative signals (backwards wheel signals)

# Network architecture (hidden layer sizes)
actor_hidden_sizes: [128, 128]   # e.g., [32, 32] means input -> 32 -> 32 -> output
critic_hidden_sizes: [512, 512]  # Both critics share this architecture

# Past states for temporal awareness
past_states:
  enabled: true  # Enable past state augmentation
  source: "sensors"  # "wheels" or "sensors" - wheel outputs are more concise 
  count: 8  # Number of past states to include
  stride: 3  # Stride between past states (e.g., t-3, t-6, t-9, t-12 if stride=3)
